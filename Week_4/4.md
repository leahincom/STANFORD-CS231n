## Backpropagation

`f` will correspond to

- the loss function `L`

- inputs ( `x` ) 

  - `training data` : given and fixed

  - the neural network weights `W` and bias `b` : variables we have control over

    > usually only compute the gradient for the parameters `W`,`b` : use it to perform a parameter update
    >
    > the gradient on `xi` : visualization and interpreting what the Neural Network might be doing



### Interpretation of the Gradient

> the gradient `âˆ‡f` is the vector of partial derivatives
>
> the partial derivative on x == *â€œthe gradient on xâ€* 

the `derivative` on each variable tells you the `sensitivity` of the whole expression according to its value

* f(x,y)=max(x,y) â†’ âˆ‚f/âˆ‚x=ðŸ™(x>=y), âˆ‚f/âˆ‚y=ðŸ™(y>=x)



### Chain Rule

> always assume that the gradient is computed on the final output

> The **backward pass** then performs backpropagation which starts at the end and recursively applies the chain rule to compute the gradients (shown in red) all the way to the inputs of the circuit.



### Backpropagation

gates communicating to each other (through the gradient signal) whether they want their outputs to increase or decrease (and how strongly), so as to make the final output value ` higher`

![image-20210403215059148](/Users/JungHyunLah/Library/Application Support/typora-user-images/image-20210403215059148.png)

1. the gate will eventually learn about the gradient of its output value on the final output of the entire circuit.

   > `+`
   >
   > * gradient of its output value on the final output : -4
   >
   > `*`
   >
   > * gradient of its output value on the final output : 1

2. take that gradient and multiply it into every gradient it normally computes for all of its inputs.

   > `+`
   >
   > * making the gradient on both **x** and **y** 1 * -4 = -4



### Modularity : Sigmoid

![image-20210403220455289](/Users/JungHyunLah/Library/Application Support/typora-user-images/image-20210403220455289.png)

![image-20210403220505382](/Users/JungHyunLah/Library/Application Support/typora-user-images/image-20210403220505382.png)

![image-20210403220515031](/Users/JungHyunLah/Library/Application Support/typora-user-images/image-20210403220515031.png)



#### Staged Computation

![image-20210403220530861](/Users/JungHyunLah/Library/Application Support/typora-user-images/image-20210403220530861.png)

> `decomposing` : break down the forward pass into stages that are easily backpropped through.



```python
w = [2,-3,-3] # assume some random weights and data
x = [-1, -2]

# forward pass
dot = w[0]*x[0] + w[1]*x[1] + w[2]
f = 1.0 / (1 + math.exp(-dot)) # sigmoid function

# backward pass through the neuron (backpropagation)
ddot = (1 - f) * f # gradient on dot variable, using the sigmoid gradient derivation
dx = [w[0] * ddot, w[1] * ddot] # backprop into x
dw = [x[0] * ddot, x[1] * ddot, 1.0 * ddot] # backprop into w
# we're done! we have the gradients on the inputs to the circuit
```



#### Further Implementation...

1. Cache forward pass variables
   * To compute the backward pass it is very helpful to have some of the variables that were used in the forward pass.
2. Gradients add up at forks
   * The forward expression involves the variables **x,y** multiple times, so when we perform backpropagation we must be careful to use `+=` instead of `=` to accumulate the gradient on these variables



### Patterns in Backward Flow

1. Sum operation (gate)

   * distributes gradients equally to all its inputs

   > - local gradient for the add operation is simply +1.0
   >
   > - it will be multiplied by x1.0
   >
   > - so the gradients on all inputs will exactly equal the gradients on the output

2. Max operation

   * routes the gradient to the higher input

   > - the local gradient for a max gate is 1.0 for the highest value, and 0.0 for all other values
   >
   > - the max gate distributes the gradient (unchanged) to exactly one of its inputs (the input that had the highest value during the forward pass)

3. Multiply operation

   * takes the input activations (values), swaps them and multiplies by its gradient

   > - local gradients are the input values (except switched)
   > - multiplied by the gradient on its output



#### Preprocessing

1. multiplied all input data examples xi by `n`

2. gradient on the weights will be `n` times larger

3. *should lower the learning rate by `n` to compensate*



### Gradients for Vectorized Operations

#### Matrix-Matrix Multiply Gradient

```python
# forward pass
W = np.random.randn(5, 10)
X = np.random.randn(10, 3)
D = W.dot(X)

# now suppose we had the gradient on D from above in the circuit
dD = np.random.randn(*D.shape) # same shape as D
dW = dD.dot(X.T) #.T gives the transpose of the matrix
dX = W.T.dot(dD)
```

