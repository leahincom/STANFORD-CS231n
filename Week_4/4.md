## Backpropagation

### Computational Graph

`f` will correspond to

- the loss function `L`

- inputs ( `x` ) 

  - `training data` : given and fixed

  - the neural network weights `W` and bias `b` : variables we have control over

    > usually only compute the gradient for the parameters `W`,`b` : use it to perform a parameter update
    >
    > the gradient on `xi` : visualization and interpreting what the Neural Network might be doing



#### Modularized Implementation

<img width="640" alt="Screenshot 2021-04-04 at 22 07 24" src="https://user-images.githubusercontent.com/49134038/113510575-82619b80-9596-11eb-809a-65ea0ff5f0aa.png">

1. forward pass ~= `forward API`
2. backward pass ~= `backward API`

<img width="639" alt="Screenshot 2021-04-04 at 22 09 28" src="https://user-images.githubusercontent.com/49134038/113510574-81c90500-9596-11eb-97d0-22c602db1b7a.png">



### Interpretation of the Gradient

> the gradient `∇f` is the vector of partial derivatives
>
> the partial derivative on x == *“the gradient on x”* 

the `derivative` on each variable tells you the `sensitivity` of the whole expression according to its value

* f(x,y)=max(x,y) → ∂f/∂x=𝟙(x>=y), ∂f/∂y=𝟙(y>=x)



### Chain Rule

* always assume that the gradient is computed on the final output
* The **backward pass** then performs backpropagation which starts at the end and recursively applies the chain rule to compute the gradients (shown in red) all the way to the inputs of the circuit.



#### Jacobian Matrix

<img width="300" alt="Screenshot 2021-04-04 at 20 48 05" src="https://user-images.githubusercontent.com/49134038/113510578-8392c880-9596-11eb-875b-1bb0d022e24e.png">

<img width="739" alt="Screenshot 2021-04-04 at 20 48 41" src="https://user-images.githubusercontent.com/49134038/113510577-82fa3200-9596-11eb-95a5-dea53e5f0cbb.png">

* size = [n * n!]

  > input size : n
  >
  > output size : 1



### Backpropagation

gates communicating to each other (through the gradient signal) whether they want their outputs to increase or decrease (and how strongly), so as to make the final output value ` higher`

<img width="436" alt="Screenshot 2021-04-03 at 21 50 54" src="https://user-images.githubusercontent.com/49134038/113510638-b5a42a80-9596-11eb-9458-45675a0c7bbc.png">

1. the gate will eventually learn about the gradient of its output value on the final output of the entire circuit.

   > `+`
   >
   > * gradient of its output value on the final output : -4
   >
   > `*`
   >
   > * gradient of its output value on the final output : 1

2. take that gradient and multiply it into every gradient it normally computes for all of its inputs.

   > `+`
   >
   > * making the gradient on both **x** and **y** 1 * -4 = -4



### Modularity : Sigmoid

<img width="277" alt="Screenshot 2021-04-03 at 22 04 52" src="https://user-images.githubusercontent.com/49134038/113510636-b5a42a80-9596-11eb-97d7-1a6ae6ffa37e.png">

<img width="444" alt="Screenshot 2021-04-03 at 22 05 02" src="https://user-images.githubusercontent.com/49134038/113510635-b50b9400-9596-11eb-8dd4-6e5b90b25b22.png">

<img width="701" alt="Screenshot 2021-04-03 at 22 05 12" src="https://user-images.githubusercontent.com/49134038/113510634-b472fd80-9596-11eb-9822-e1c9fa1fb8e3.png">



#### Staged Computation

<img width="663" alt="Screenshot 2021-04-03 at 22 05 24" src="https://user-images.githubusercontent.com/49134038/113510632-b210a380-9596-11eb-97ed-e8f1db946bb2.png">

> `decomposing` : break down the forward pass into stages that are easily backpropped through.



```python
w = [2,-3,-3] # assume some random weights and data
x = [-1, -2]

# forward pass
dot = w[0]*x[0] + w[1]*x[1] + w[2]
f = 1.0 / (1 + math.exp(-dot)) # sigmoid function

# backward pass through the neuron (backpropagation)
ddot = (1 - f) * f # gradient on dot variable, using the sigmoid gradient derivation
dx = [w[0] * ddot, w[1] * ddot] # backprop into x
dw = [x[0] * ddot, x[1] * ddot, 1.0 * ddot] # backprop into w
# we're done! we have the gradients on the inputs to the circuit
```



#### Further Implementation...

1. Cache forward pass variables
   * To compute the backward pass it is very helpful to have some of the variables that were used in the forward pass.
2. Gradients add up at forks
   * The forward expression involves the variables **x,y** multiple times, so when we perform backpropagation we must be careful to use `+=` instead of `=` to accumulate the gradient on these variables



### Patterns in Backward Flow

1. Sum operation (gate)

   * distributes gradients equally to all its inputs

   > - local gradient for the add operation is simply +1.0
   >
   > - it will be multiplied by x1.0
   >
   > - so the gradients on all inputs will exactly equal the gradients on the output

2. Max operation

   * routes the gradient to the higher input

   > - the local gradient for a max gate is 1.0 for the highest value, and 0.0 for all other values
   >
   > - the max gate distributes the gradient (unchanged) to exactly one of its inputs (the input that had the highest value during the forward pass)

3. Multiply operation

   * takes the input activations (values), swaps them and multiplies by its gradient

   > - local gradients are the input values (except switched)
   > - multiplied by the gradient on its output



#### Preprocessing

1. multiplied all input data examples xi by `n`

2. gradient on the weights will be `n` times larger

3. *should lower the learning rate by `n` to compensate*



### Gradients for Vectorized Operations

#### Matrix-Matrix Multiply Gradient

<img width="264" alt="Screenshot 2021-04-04 at 22 01 22" src="https://user-images.githubusercontent.com/49134038/113510576-82619b80-9596-11eb-9b83-1390f1813576.png">

> 1_k=i * x_j
>
> * if k=i, 1
> * else x_j

* Gradient with respect to a variable should have the same shape as the variable

  👉 sanity check

```python
# forward pass
W = np.random.randn(5, 10)
X = np.random.randn(10, 3)
D = W.dot(X)

# now suppose we had the gradient on D from above in the circuit
dD = np.random.randn(*D.shape) # same shape as D
dW = dD.dot(X.T) #.T gives the transpose of the matrix
dX = W.T.dot(dD)
```



## Neural Network

1. Linear Score Function `f = Wx`

👇  __*Stack multiple layers (f=Wx)*__

2. 2-layer Neural Network `f = W2 * max(0, W1x)`

   > non-linear function

<img width="317" alt="Screenshot 2021-04-04 at 22 15 14" src="https://user-images.githubusercontent.com/49134038/113510573-81c90500-9596-11eb-8cd6-312dd58e08f6.png">

* h = W1 * x = max(0, W1x)
* s = W2 * h = f
* `template` == each row of the score function



<img width="572" alt="Screenshot 2021-04-04 at 22 19 06" src="https://user-images.githubusercontent.com/49134038/113510572-81306e80-9596-11eb-951b-fd8a0f75095c.png">

**car template**

`x` : red car

`W1` : red car template 👉 `max` on top : *non-linearity*

`h` : the scores of each row from the template (`W1`)

`W2` : *(weighted sum of all the intermediate scores)* red + yellow car template

`s` : **final score == ** red + yellow car score



#### Idea

<img width="647" alt="Screenshot 2021-04-04 at 22 30 11" src="https://user-images.githubusercontent.com/49134038/113510569-8097d800-9596-11eb-9dd5-d12408a1865f.png">

<img width="347" alt="Screenshot 2021-04-04 at 22 30 41" src="https://user-images.githubusercontent.com/49134038/113510568-7fff4180-9596-11eb-8dee-ee6cb881a18b.png">

> feed-forward computation of a neural network



#### Activation Function

1. Sigmoid
2. tanh
3. ReLU
4. Leaky ReLU
5. Maxout
6. ELU



#### Architectures

<img width="645" alt="Screenshot 2021-04-04 at 22 33 18" src="https://user-images.githubusercontent.com/49134038/113510566-7ece1480-9596-11eb-8f85-3175f582b324.png">

1. input layer
2. hidden layer
3. output layer

> `FC` : Fully-Connected layers

👉 n-layers-network == (n-1)-hidden-layers-network

<img width="500" alt="Screenshot 2021-04-04 at 22 35 34" src="https://user-images.githubusercontent.com/49134038/113510562-7b3a8d80-9596-11eb-82ac-2b5641dfd12b.png">

> f : sigmoid function

