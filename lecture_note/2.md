## Lecture2

### Image Classification Pipeline

pre-determined set of labels -> assign one of the label



#### Problems

- Illumination : lighting condition

- Deformation
- Occlusion
- Background clutter
- Intraclass variation in ```color, shape, etc```

>  e.g.

>  Semantic Gap : difference in pixel values

> + when camera changes -> pixel changes

👇

*hard to predicting simultaneously*



### An image classifier

1. find edges

2. find corners

*compute the edges 👉 find corners*

>  problem : start over again with diff objects



### Data-driven Approach

1. large dataset ( e.g. google image search )

2. machine learning classifier

3. evaluate

👇

two functions : ```train``` + ```predict```

1. train : input images + labels
2. predict : input model 👉 output prediction



#### First classifier : Nearest Neighbor

- memorize all the data and labels

- find the most similar training image

##### Example Dataset: CIFAR10

##### Distance Metric to compare images : L1 Distance

> sum up the absolute values of the points



> Q. With N examples, how fast are training and prediciton?
>
> A. Train O(1), predict O(N)

❗️it should be opposite : fastify a prediction (training doesn't matter)

❗️wanna run on mobile, or any device

❗️wrong decision regions



### K-Nearest Neighbors

take majority vote from K closest points

👉 smoothe out the decision boundary



> each pixel gathered => picture itself : high dimensional vector



#### Distance Metric

*__specifying each data into ```different distance metrics```__*

*__👉 can ```compare``` any type of data__*

1. L1 (Manhattan) distance

   * **coordinate dependency** 👉 changing coordinate point matters

   * if individual point has its own meaning

2. L2 (Euclidean) distance

   * don't know the actual meaning of individual points



![L1_L2](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fo1iTy%2FbtqCOfgbW76%2Fxv5PL4KWIDdrHAnvXdzTKK%2Fimg.png)

>  green : Euclidean, L2 distance
>
> others : L1 distance



#### HyperParameters

> K, distance metric, etc...

* not necessarily learned from the training data 

* no way to learn directly from the data
* **problem-dependent** 👉 try different values to find out the best choice

* **choices about the algorithm that we set rather than learn**
* e.g. try both L1, L2 distance and choose



##### Setting Hyperparameters

**Idea1** Choosing hyperparameters that work best on the full dataset

💥 K=1 always works the best, but makes the worst output to the unseen dataset



**Idea2** Splitting data into train and test; choose the best one for the test data

💥 Same with choosing one kind of answer that best fits to the test data, again can't be applied to the unseen dataset



**Idea3** Splitting data into train, validation, test data; choose the best one for the test data, and again debug things in the test data

> can't access to the validation dataset, only can check the labels if the algorithm is doing well

> equally distributing the dataset to make the test data representative the wild data
>
> : use the same methodology to collect the data, randomly distributing that to make well-distributed separate dataset



**Idea4** Cross-Validation; splitting into train, test dataset; again splitting the train dataset into multiple folds and cycle through all the fold datasets, choose the best hyperparameter and go into the test dataset

💥 impractical for deep learning problems -> too large data to use this



#### Problem

1. distance between pixels sometimes can not represent perceptual differences, can not capture the full differences between images because it only considers the single distance metric

2. curse of dimensionality

   * use each data as the partition of the space 

   * need many data to cover the high dimensional space 👉 otherwise, not similar to real data

   

***

   

### Linear Classification

the basic for Neural Networks

```linear classification``` : lego block

```neural network``` : lego building



### Parametric Approach



> **Difference between KNN and parametric approach**
>
> 1. KNN : data-driven 👉 use data in test
> 2. parametric approach : summarize the knowledge of the data into W and use it in test 👉 time-efficient



1. ```x``` : input image, works as a template

2. ```W``` : weights / parameters / data

3. ```f``` : combining the ```x``` with data

   in this case, **```f``` => linear classifier** : f(x, W) = Wx

   > f(x, W) = Wx + b
   >
   > => ```b``` : bias towards the expecting class result

```
// x : 32 * 32 * 3
// f(x, W) : 10 * 1

Task : make 10 class of scores(f) from input(x)
Answer : need 10 classes(images) with size 32 * 32 * 3,
W = 10 * 3072
```



![linear](http://aikorea.org/cs231n/assets/imagemap.jpg)

```input image``` : 2 * 2 pixel --> x = 4 * 1

```class (different pictures)``` : 3 classes with 2 * 2 pixels --> W = 3 * 4

```bias``` : 



*template matched score == larger possibility to be the answer*

🚀 coming out with the best ```f``` is the actual task for the deep learning



#### Problem

1. learn from only one template per category

2. one linear classifier -> seperate into only two categories,

   thus if there are more than 2 categories, one linear is not enough to work fully as the classifier

   * 4 quarters
   * circle catogory with 'others' category

   * multi-modal

