## Lecture2

### Image Classification Pipeline

```train``` : pre-determining set of labels

-> ```predict``` : assign one label among training dataset labels



#### Problems

- Illumination : lighting condition

- Deformation
- Occlusion
- Background clutter
- Intraclass variation in ```color, shape, etc```

>  e.g.

>  Semantic Gap : difference in pixel values

> + when camera changes -> pixel changes

👇

*hard to predicting simultaneously*



### An image classifier

1. find edges

2. find corners

*compute the edges 👉 find corners*

>  problem : start over again with diff objects



### Data-driven Approach

1. large dataset ( e.g. google image search )

2. machine learning classifier

3. evaluate

👇

two functions : ```train``` + ```predict```

1. train : input images + labels
2. predict : input model 👉 output prediction



#### First classifier : Nearest Neighbor

- ```train``` : memorize all the data and labels = O(1)

- ```predict``` : find the most similar training image = O(n)

##### Example Dataset: CIFAR10

##### Distance Metric to compare images : L1 Distance

> sum up the absolute values of the points



> Q. With N examples, how fast are training and prediciton?
>
> A. Train O(1), predict O(N)

❗️it should be opposite : fastify a prediction (training doesn't matter)

❗️wanna extend to mobile, or any device

❗️wrong decision regions



### K-Nearest Neighbors

take majority vote from K closest points

👉 smoothe out the decision boundary



![kNN](https://user-images.githubusercontent.com/40735375/50537854-fc7d6b80-0ba8-11e9-9045-937ce1884a5c.png)

k = 1; 하나의 데이터에 의존해서 미세한 변화에도 쉽게 영향을 받게된다.

k = 3; 초록색의 영역 한 가운데에 있던 노란색의 영역이 사라진 것을 확인할 수 있다. 그럼에도 아직 빨강-파랑의 경계는 들쑥날쑥한 것을 알 수 있다.

k = 5; 들쑥날쑥하던 경계도 사라진 것을 확인할 수 있다.



#### Distance Metric

*__specifying each data into ```different distance metrics```__*

*__👉 can ```compare``` any type of data__*

1. L1 (Manhattan) distance

   * **coordinate dependency** 👉 changing coordinate point matters

   * if every partition in points has its own meaning

2. L2 (Euclidean) distance

   * don't know the actual meaning of individual partitions
   * but if the feature(point) has the meaning in its space



![L1_L2](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fo1iTy%2FbtqCOfgbW76%2Fxv5PL4KWIDdrHAnvXdzTKK%2Fimg.png)

>  green : Euclidean, L2 distance
>
> others : L1 distance



👇

```k의 값을 무엇으로 설정하느냐```에 따라서 모델이 내놓는 결과 값이 달라지게 된다. 그리고 두 가지의 거리 공식 중에서도 ```어떤 거리 공식을 사용하느냐```에 따라서 결과 값이 달라지게 된다. 이렇듯 ```데이터로부터 학습을 통해 얻어지는 파라미터들이 아니라,``` 알고리즘과 데이터의 성격에 따라서 ```우리가 결정해야 하는 파라미터들```을 __*hyperparameters*__라고 한다. 

문제의 성격에 따라서 적절한 하이퍼파라미터가 매우 달라지기 때문에 여러가지를 실험해보고 가장 성능이 좋은 하이퍼파라미터를 선택해야 한다.

***



#### HyperParameters

> K, distance metric, etc...

* training을 위해 미리 설정, 전달하는 parameter

* not necessarily learned from the training data

* no way to learn directly from the data
* **problem-dependent** 👉 try different values to find out the best choice

* **choices about the algorithm that we set rather than learn**
* e.g. try both L1, L2 distance and choose



##### Setting Hyperparameters

**Idea1** Choosing hyperparameters that work best on the full dataset

💥 ```overfitting``` : K=1 always works the best, but makes the worst output to the unseen dataset; 모형은 만들어냈지만, 모형의 성능을 ```평가할 수 없다```. 기존의 데이터 셋에 대해서는 완벽하게 작동하겠지만, 생성된 모형이 좋은지 안좋은지 알 수 없다.



**Idea2** Splitting data into train and test; choose the best one for the test data

💥 Same with choosing one kind of answer that best fits to the test data, again can't be applied to the unseen dataset; 평가를 위해 사용해야할 테스트 셋이 ```하이퍼파라미터 적합에 사용된다```. testing set에 대한 예측결과를 보고 하이퍼파라미터를 조절하기 때문에 testing set 역시 학습에 이용하는 셈이 된다. 따라서 더 이상 testing set에 대한 결과가 완전히 새로운 데이터에 대한 결과를 대표하지 못하게 된다.

> - **1)** Split the data at hand into training and test subsets
> - **2)** Repeat optimization loop a fixed number of times or until a condition is met:
>   - **a)** Select a new set of model hyperparameters
>   - **b)** Train the model on the training subset using the selected set of hyperparameters
>   - **c)** Apply the model to the test subset and generate the corresponding predictions
>   - **d)** Evaluate the test predictions using the appropriate scoring metric for the problem at hand, such as accuracy or mean absolute error. Store the metric value that corresponds to the selected set of hyperparameters
> - **3)** Compare all metric values and choose the hyperparameter set that yields the best metric value



**Idea3** Splitting data into train, validation, test data; choose the best one for the test data, and again debug things in the test data

![idea3](https://losskatsu.github.io/assets/images/ml/validation/validation05.jpg)

모형의 ```파라미터 추정에는 트레이닝셋을``` 사용하고, ```하이퍼파라미터 설정에는 밸리데이션 셋을``` 사용한다. 그리고 마지막으로 ```테스트셋```에 모형을 적용시켜 단 한번만 실험한 후 정확도를 측정한다.

> can't access to the validation dataset, only can check the labels if the algorithm is doing well

> equally distributing the dataset to make the test data representative the wild data
>
> : use the same methodology to collect the data, randomly distributing that to make well-distributed separate dataset



**Idea4** Cross-Validation; splitting into train, test dataset; again splitting the train dataset into multiple folds and cycle through all the fold datasets, choose the best hyperparameter and go into the test dataset

> 데이터셋이 적고, unbalanced data일 경우 좋은 성능을 보인다.

![val](https://losskatsu.github.io/assets/images/ml/validation/validation06.jpg)

먼저 데이터를 training set과 testing set으로 나눈 후에, training set을 k개로 나누어서 각각의 조각에 대해서 한번씩 validation으로 사용하면서 학습을 진행한다; 트레이닝을 k등분하고 validation set을 여러번 바꿔가며 반복적으로 시행하는 방법을 k-fold cross validation이라고 한다.

![result](https://cs231n.github.io/assets/cvplot.png)

5-fold cross-validation accuracy with different k value for kNN,

5 results (cases) for each k value (5-fold)

![overview](https://losskatsu.github.io/assets/images/ml/validation/validation07.jpg)

💥 데이터셋의 절대적인 양이 적을 때 주로 사용하지만, impractical for deep learning problems; too large data to use this, high computational cost



#### Problem

1. distance between feature vectors (represents each picture) sometimes can not represent perceptual differences, can not capture the full differences between images because it only considers the single distance metric.

   하지만 각 픽셀간의 거리를 구한 후에 그 거리의 총합이 적을 수록 이미지가 비슷할 것이라는 추론은 언뜻 듣기에도 합리적이지 않다. kNN의 아이디어 자체가 이미지 데이터의 특성과는 거리가 멀기 때문에 kNN으로 이미지를 분류하지는 않는다.

![prob1](https://www.pyimagesearch.com/wp-content/uploads/2016/08/knn_animal_clusters.jpg)

> kNN classifier : dist between blue-red circle >>> dist between pics inside the  circles



2. curse of dimensionality

   > each pixel : each vector is gathered
   >
   > => picture itself : high dimensional vector

   * use each data as the partition of the space 

   * need many data to cover the high dimensional space 👉 otherwise, not similar to real data

![prob2](https://images.deepai.org/glossary-terms/curse-of-dimensionality-61461.jpg)

> The volume of the space represented grows so quickly, but the amount of the data is the same. 
>
> As the data space seen above moves from one dimension to two dimensions and finally to three dimensions, the given data fills less and less of the data space. 

  

3. Takes too much time because it takes all the images as the input data for training

***

   

### Linear Classification

the basic for Neural Networks

```linear classification``` : lego block

```neural network``` : lego building



![li](https://t1.daumcdn.net/cfile/tistory/999181485B5820280D)

xi는 이미지를 1차원의 vector로 만든 형태이며, yi는 그 이미지의 label을 가리키는 값이라고 하자. (10 class면, one hot encoding으로 하나 값을 갖게 된다.) linear classifier는 입력과 출력 사이의 관계를 '선형'으로 나타낸 것이다.

W는 각 class의 classifier를 학습을 하는 것이다. 즉, W의 각 행은 각각의 class를 구분하는 것을 학습하는 것이다. Bias는 이 선들이 평행이동을 하도록 해준다. (대부분의 linear classifier는 어떤 class와 그 외 class를 구분짓는 'hyperplane'을 찾는 것을 목표로 한다. 그리고 bias는 이 선을 평행이동을 시키는 것이다.)

template와 매칭되는 값을 학습한다는 것은, 어떤 class를 분류할 때 그 class와 비슷하도록 weight들이 학습이 된다는 것이다. 만약 이 class와 비슷한 그림이 오면 그 weight은 잘 반응 (값이 큼)해서 (activation) score를 높인다. 딥러닝에서는 이 template이 스스로 학습이 된다.



### Parametric Approach



> **Difference between KNN and parametric approach**
>
> 1. KNN : data-driven 👉 use data in test
>
> 2. parametric approach : summarize [training] the knowledge of the data into W and use it in test 👉 time-efficient
>
>    linear classifier의 경우에는 학습을 통해서 parameter가 정해진다는 점이 앞서 배웠던 kNN과는 다르다.



1. ```x``` : input image, works as a template

2. ```W``` : weights / parameters / data

3. ```f``` : combining the ```x``` with data

   in this case, **```f``` => linear classifier** : f(x, W) = Wx

   > f(x, W) = Wx + b
   >
   > => ```b``` : bias towards the expecting class result

```
32x32 픽셀 사이즈를 가지는 RGB이미지를 10개의 클래스로 분류하는 작업을 시도한다고 하자. 이 때 우리가 최종적으로 구하려고 하는 것은 10개의 클래스에 대한 값이므로 f(x,W) 는 10x1의 크기가 될 것이다. 그리고 인풋은 이미지 한 장이므로 3072(32*32*3)x1 이다.
// x : 32 * 32 * 3 = 3072 * 1
// f(x, W) : 10 * 1

따라서 W의 적절한 크기는 10x3072가 된다.
b 는 biased term으로 f(x,W) 과 동일한 10x1의 크기를 가진다.
Task : make 10 class of scores(f) from input(x)
Answer : need 10 classes(images) with size 32 * 32 * 3,
W = 10 * 3072, b = 10 * 1
```



![linear](http://aikorea.org/cs231n/assets/imagemap.jpg)

```input image``` : 2 * 2 pixel --> x = 4 * 1

```class (different pictures)``` : 3 classes with 2 * 2 pixels --> W = 3 * 4

```bias``` : 



*higher score == larger possibility to be the answer*

🚀 coming out with the best ```f``` is the actual task for the deep learning



#### Problem

1. learn from only one template per category

2. one linear classifier -> seperate into only two categories,

   thus if there are more than 2 categories, one linear is not enough to work fully as the classifier

   * 4 quarters
   * circle catogory with 'others' category

   * multi-modal

