## Linear Classification

1. stretch out the image into column vector

2. make it into 10*1 column vector having scores to each class

3. larger score == closer to the answer



#### What to learn

* **score function** that maps the raw data to class scores
  * how much each pixel will influence the class
  * learning templates per class
  * each row = a template for the class
  * dimension of the space = pixel intensity value


* **loss function** that quantifies the agreement between the predicted scores and the ground truth labels.
  * how to actually choose W and use the data
  * quantify how bad the W is => loss function



## Loss Function

* can discard the training set
* only uses the learned parameters (W, b) within the classifier function
* the loss will be high if weâ€™re doing a poor job of classifying the training data // it will be low if weâ€™re doing well.



_**how good or bad the W is**_

1. choose W that minimizes the loss (correct value: the least bad)

2. optimization procedure



`W`

* weight W == parameters

* (K, D)
  * each row == one classifier
* while (x, y) is given, we can control W, b -- which should be optimized to match the ground truth labels



### multi-class SVM Loss

*the SVM loss function wants the score of the correct class yi to be larger than the incorrect class scores by at least by Î” (delta).*

![img](https://cs231n.github.io/assets/margin.jpg)



**sum over all the loss**

`s` : predicted scores for the classes

`s_yi` = true category

`s_j` = false category

`Î”` = safety margin : classë¥¼ ê°™ê±°ë‚˜ ë‹¤ë¥´ë‹¤ê³  êµ¬ë¶„í•˜ëŠ” margin ê°’

i.e. Î” = 10; the difference is at least 10, Any additional difference above the margin (negative value) is clamped at zero with the max operation.

>  _we want to have ... "much higher score for the true class"_



* relative difference between scores

* correct score is much greater than the incorrect scores
  * quantify how bad are different mistakes (s_j)
  * how much we care about different categories of errors

* scaling up or down the entire W == rescale all the scores correspondingly

* weigh off different trade-offs of different types of mistakes



### hinge loss

![Screenshot 2021-03-26 at 19.54.02](/Users/JungHyunLah/git/STANFORD-CS231n/Week_3/Screenshot 2021-03-26 at 19.54.02.png)

- x axis = s_yi / s_j == true class
- y axis = loss

  * more true = less loss
  * at the end: loss = 0; already classified true value

***

**Debug** using  `C-1`

***



fitting to training data < **performance on the test data**

*Occam's Razor : 'prefer the simple one'*

ðŸ‘‡

## Regularization

*simplify the W, penalizing the ambiguity of the model rather than explicitely fit the training data*

>  choose the type of regularization algo : problem-dependent



### L2 Regularization

![image-20210329105647437](/Users/JungHyunLah/Library/Application Support/typora-user-images/image-20210329105647437.png)

![image-20210329105701433](/Users/JungHyunLah/Library/Application Support/typora-user-images/image-20210329105701433.png)

>  *There is no simple way of setting this hyperparameter and it is usually determined by cross-validation.*

* lambda : control the trade-off between data loss and optimization loss

  * _**important hyper-parameter to tune!**_



> 1. hard constraint : constraint to contain high dimensional model
>
> 2. **soft constraint** : if you want to contain high dimensional model, it has to overcome the penalty

* penalizing the Euclidean norm of W (weight vector)

* squared norm

* measuring the model complexity by **how much the values are spreaded over all the entries which means less complex**

* opposite measuring criteria to L1 regularization

* the L2 penalty prefers smaller and more diffuse weight vectors
  * e.g. prefer  [0.25,0.25,0.25,0.25] > [1,0,0,0]
  * the final classifier is encouraged to take into account all input dimensions to small amounts
    * improve the generalization performance of the classifiers on test images and lead to less *overfitting*.



### L1 Regularization

* encouraging sparsity

* measuring the model complexity by **measuring the number of 0 entries(values) in the weight vector**
  * e.g. prefer [1,0,0,0]  > [0.25,0.25,0.25,0.25]



### ETC...

##### Elastic Net (L1 + L2)

##### Max Norm Regularization

##### Dropout

* specific to deep learning



### Softmax Classifier : Multinomial Logistic Regression

> *binary Logistic Regression classifier for multiple classes*

![image-20210329113946114](/Users/JungHyunLah/Library/Application Support/typora-user-images/image-20210329113946114.png)

![image-20210329114041904](/Users/JungHyunLah/Library/Application Support/typora-user-images/image-20210329114041904.png)

* **softmax function**: It takes a vector of arbitrary real-valued scores (in z) and squashes it to a vector of values between zero and one that sum to one
* put much weight onto the scores
  * the function mapping f(xi;W)=Wx stays unchanged, but we now interpret these scores as the unnormalized log probabilities for each class

> * answer probability : want the probability of the true class to be 1
>
> * loss probability : want it to be closer to 0

![image-20210329114150439](/Users/JungHyunLah/Library/Application Support/typora-user-images/image-20210329114150439.png)

* The *cross-entropy* between a â€œtrueâ€ distribution p and an "estimated" distribution q 
  * p : all probability mass is on the correct class (i.e. p=[0,â€¦1,â€¦,0] contains a single 1 at the yi -th position.)
  * q : q=e^f_yi / âˆ‘j(e^f_j) (softmax function)
* the cross-entropy objective *wants* the predicted distribution to have all of its mass on the correct answer
  * == minimizing the negative log likelihood of the correct class
    * which can be interpreted as performing *Maximum Likelihood Estimation* (MLE)



> <img src="/Users/JungHyunLah/Library/Application Support/typora-user-images/image-20210329115245993.png" alt="image-20210329115245993" style="zoom:50%;" />
>
> A common choice for C is to set `logC=âˆ’max_j(f_j)`
>
> ```python
> # instead: first shift the values of f so that the highest number is 0:
> f -= np.max(f) # f becomes [-666, -333, 0]
> p = np.exp(f) / np.sum(np.exp(f)) # safe to do, gives the correct answer
> ```



![image-20210326205520601](/Users/JungHyunLah/Library/Application Support/typora-user-images/image-20210326205520601.png)

> in practice, it will never get the infinite probability cuz that of true class can't have the 0 probability



***



**Debug** using  `logC`



> to measure the badness of the W, uses margin difference between correct and incorrect class



## SVM vs. Softmax

`SVM classifier` uses the *hinge loss*, or also sometimes called the *max-margin loss*.

> the SVM is happy once the margins are satisfied and it does not micromanage the exact scores beyond this constraint.



 `Softmax classifier` uses the *cross-entropy loss*.

> the Softmax classifier is never fully happy with the scores it produces: the correct class could always have a higher probability and the incorrect classes always a lower probability and the loss would always get better. 

* if the regularization strength Î» was higher,
* the weights W would be penalized more
* and this would lead to smaller weights
* where the probabilites are now more diffuse



## Supervised Learning

![image-20210326210605027](/Users/JungHyunLah/Library/Application Support/typora-user-images/image-20210326210605027.png)

>  find the W that minimizes the loss



## Optimization



### Random Search

check all the possibilities



### Local Optimal = Slope

which way will be more downhill



**gradient** : same type of vector with x, partial derivatives

-> update the parameter values

1. positive : increase
2. negative : decrease

* do not compute it when it's not needed
* use analytic gradient : notation and actually calculate gradients when needed



**Debug** using  numerical `gradient`



## Gradient Descent

update weights to the opposite direction of the gradient descent to converge the model



* one of the most important hyperparamter

`Step Size` = Learning Rate

how far to update



can't iterate through millions of train dataset over and over again to update the W value for one time (high computing cost + time efficiency), so to speed up the process of updating the W :

### Stochastic Gradient Descent (SGD)

* minibatch (32, 64, 128) -> use as the estimate



## Image Feature

1. pull out image features as groups,

2. concatenate them to make one image feature vector (feature representation vector),
3. feed it to the linear classifier (rather than feeding the raw pixels)



### Color Histogram

divide the image using colours



### Histogram of Oriented Gradients (HoG)

divide the pixel with dominant orientation



### Bag of Words (Book of Visual Words : CodeBook)

1. get sample images, sample them into a bunch of tiny random crops,
2. cluster them using K means, etc to extract the main feature (e.g. color, orientation, etc.) of that crop (visual word)



**Here comes the ConvNet**

* rather than just writing down the features

  = extracting features, concatenating them, and updating the linear classifier which is on the top and not the feature extractor itself,

* learn the features from the data

  = feed the raw data to the convolutional network, come up with the layers which are feature representations, train the weights using these to train the entire network