## Linear Classification

1. stretch out the image into column vector

2. make it into 10*1 column vector having scores to each class

3. larger score == closer to the answer



#### What to learn

* **score function** that maps the raw data to class scores
  * how much each pixel will influence the class
  * learning templates per class
  * each row = a template for the class
  * dimension of the space = pixel intensity value


* **loss function** that quantifies the agreement between the predicted scores and the ground truth labels.
  * how to actually choose W and use the data
  * quantify how bad the W is => loss function



## Loss Function

![image-20210401201112243](/Users/JungHyunLah/Library/Application Support/typora-user-images/image-20210401201112243.png)

* can discard the training set
* only uses the learned parameters (W, b) within the classifier function
* the loss will be high if we’re doing a poor job of classifying the training data // it will be low if we’re doing well.



_**how good or bad the W is**_

1. choose W that minimizes the loss (correct value: the least bad)

2. optimization procedure



`W`

* weight W == parameters

* (K, D)
  * each row == one classifier
* while (x, y) is given, we can control W, b -- which should be optimized to match the ground truth labels



### multi-class SVM Loss

*the SVM loss function wants the score of the correct class yi to be larger than the incorrect class scores by at least by Δ (delta).*

![img](https://cs231n.github.io/assets/margin.jpg)



**sum over all the loss**

`s` : predicted scores for the classes

`s_yi` = true category

`s_j` = false category

`Δ` = safety margin : class를 같거나 다르다고 구분하는 margin 값

i.e. Δ = 10; the difference is at least 10, Any additional difference above the margin (negative value) is clamped at zero with the max operation.

>  _we want to have ... "much higher score for the true class"_



* relative difference between scores

* correct score is much greater than the incorrect scores
  * quantify how bad are different mistakes (s_j)
  * how much we care about different categories of errors

* scaling up or down the entire W == rescale all the scores correspondingly

* weigh off different trade-offs of different types of mistakes



### hinge loss

![Screenshot 2021-03-26 at 19.54.02](/Users/JungHyunLah/git/STANFORD-CS231n/Week_3/Screenshot 2021-03-26 at 19.54.02.png)

- x axis = s_yi / s_j == true class
- y axis = loss

  * more true = less loss
  * at the end: loss = 0; already classified true value

***

**Debug** using  `C-1`

***



fitting to training data < **performance on the test data**

*Occam's Razor : 'prefer the simple one'*

👇

## Regularization

*simplify the W, penalizing the ambiguity of the model rather than explicitely fit the training data*

>  choose the type of regularization algo : problem-dependent



### L2 Regularization

![image-20210329105647437](/Users/JungHyunLah/Library/Application Support/typora-user-images/image-20210329105647437.png)

![image-20210329105701433](/Users/JungHyunLah/Library/Application Support/typora-user-images/image-20210329105701433.png)

>  *There is no simple way of setting this hyperparameter and it is usually determined by cross-validation.*

* lambda : control the trade-off between data loss and optimization loss

  * _**important hyper-parameter to tune!**_



> 1. hard constraint : constraint to contain high dimensional model
>
> 2. **soft constraint** : if you want to contain high dimensional model, it has to overcome the penalty

* penalizing the Euclidean norm of W (weight vector)

* squared norm

* measuring the model complexity by **how much the values are spreaded over all the entries which means less complex**

* opposite measuring criteria to L1 regularization

* the L2 penalty prefers smaller and more diffuse weight vectors
  * e.g. prefer  [0.25,0.25,0.25,0.25] > [1,0,0,0]
  * the final classifier is encouraged to take into account all input dimensions to small amounts
    * improve the generalization performance of the classifiers on test images and lead to less *overfitting*.



### L1 Regularization

* encouraging sparsity

* measuring the model complexity by **measuring the number of 0 entries(values) in the weight vector**
  * e.g. prefer [1,0,0,0]  > [0.25,0.25,0.25,0.25]



### ETC...

##### Elastic Net (L1 + L2)

##### Max Norm Regularization

##### Dropout

* specific to deep learning



### Softmax Classifier : Multinomial Logistic Regression

> *binary Logistic Regression classifier for multiple classes*

![image-20210329113946114](/Users/JungHyunLah/Library/Application Support/typora-user-images/image-20210329113946114.png)

![image-20210329114041904](/Users/JungHyunLah/Library/Application Support/typora-user-images/image-20210329114041904.png)

* **softmax function**: It takes a vector of arbitrary real-valued scores (in z) and squashes it to a vector of values between zero and one that sum to one
* put much weight onto the scores
  * the function mapping f(xi;W)=Wx stays unchanged, but we now interpret these scores as the unnormalized log probabilities for each class

> * answer probability : want the probability of the true class to be 1
>
> * loss probability : want it to be closer to 0

![image-20210329114150439](/Users/JungHyunLah/Library/Application Support/typora-user-images/image-20210329114150439.png)

* The *cross-entropy* between a “true” distribution p and an "estimated" distribution q 
  * p : all probability mass is on the correct class (i.e. p=[0,…1,…,0] contains a single 1 at the yi -th position.)
  * q : q=e^f_yi / ∑j(e^f_j) (softmax function)
* the cross-entropy objective *wants* the predicted distribution to have all of its mass on the correct answer
  * == minimizing the negative log likelihood of the correct class
    * which can be interpreted as performing *Maximum Likelihood Estimation* (MLE)



> <img src="/Users/JungHyunLah/Library/Application Support/typora-user-images/image-20210329115245993.png" alt="image-20210329115245993" style="zoom:50%;" />
>
> A common choice for C is to set `logC=−max_j(f_j)`
>
> ```python
> # instead: first shift the values of f so that the highest number is 0:
> f -= np.max(f) # f becomes [-666, -333, 0]
> p = np.exp(f) / np.sum(np.exp(f)) # safe to do, gives the correct answer
> ```



![image-20210326205520601](/Users/JungHyunLah/Library/Application Support/typora-user-images/image-20210326205520601.png)

> in practice, it will never get the infinite probability cuz that of true class can't have the 0 probability



***



**Debug** using  `logC`



> to measure the badness of the W, uses margin difference between correct and incorrect class



## SVM vs. Softmax

`SVM classifier` uses the *hinge loss*, or also sometimes called the *max-margin loss*.

> the SVM is happy once the margins are satisfied and it does not micromanage the exact scores beyond this constraint.



 `Softmax classifier` uses the *cross-entropy loss*.

> the Softmax classifier is never fully happy with the scores it produces: the correct class could always have a higher probability and the incorrect classes always a lower probability and the loss would always get better. 

* if the regularization strength λ was higher,
* the weights W would be penalized more
* and this would lead to smaller weights
* where the probabilites are now more diffuse



## Supervised Learning

![image-20210326210605027](/Users/JungHyunLah/Library/Application Support/typora-user-images/image-20210326210605027.png)

>  find the W that minimizes the loss



## Optimization

finding the set of parameters W to minimize the loss function



### Bad ideas

***

#### #1. Random Search

* check all the possibilities

* try out many different random weights and keep track of what works best

* start with a random **W** and then iteratively refine it, making it slightly better each time.

```python
for num in range(1000):
  W = np.random.randn(10, 3073) * 0.0001 # generate random parameters
  loss = L(X_train, Y_train, W) # get the loss over the entire training set
  if loss < bestloss: # keep track of the best solution
    bestloss = loss
    bestW = W
```



#### #2. Random "Local" Search

> <Blindfolded hiker analogy>
>
> which way will be more downhill
>
> * Local Optimal == Slope



* start out with a random `W`, generate random perturbations `δW` to it and if the loss at the perturbed `W+δW` is lower,  perform an update

```python
W = np.random.randn(10, 3073) * 0.001 # generate random starting W

for i in range(1000):
  step_size = 0.0001
  Wtry = W + np.random.randn(10, 3073) * step_size
  loss = L(Xtr_cols, Ytr, Wtry)
  if loss < bestloss:
    W = Wtry
    bestloss = loss
```



#### #3. Follwing the Gradient

> *no need to randomly search for a good direction* :
>
> we can **compute the *best* direction** along which we should change our weight vector that is mathematically **guaranteed** to be the direction of the **steepest** descend



**gradient**

* **generalization** of slope for functions that don’t take a single number but **a vector of numbers**
* **derivatives** : same type of a vector with x which is of slopes
* vector of **partial derivatives** : take *a vector of numbers* instead of a single number



##### 1. Numerical Gradient

1. iterates over all dimensions one by one, `evaluate the gradient and to perform only a single parameter update` 

   * makes a small change `h` along that dimension

   * calculates the partial derivative of the loss function along that dimension
     * see how much the function changed

```python
  # iterate over all indexes in x
  it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])
  while not it.finished:

    # evaluate function at x+h
    ix = it.multi_index
    old_value = x[ix]
    x[ix] = old_value + h # increment by h
    fxh = f(x) # evalute f(x + h)
    x[ix] = old_value # restore to previous value (very important!)

    # compute the partial derivative
    grad[ix] = (fxh - fx) / h # the slope
    it.iternext() # step to next dimension

  return grad
```

> Ideally, you want to use the smallest step size that does not lead to numerical issues.
>
> > choosing the step size (also called the *learning rate*) will become one of the most important (and most headache-inducing) hyperparameter settings in training a neural network.
>
> **centered difference formula**: [f(x+h)−f(x−h)]/2h
>
> ```python
> df = eval_numerical_gradient(CIFAR10_loss_fun, W) # get the gradient
> ```

4. update

> 1. positive : increase
> 2. negative : decrease

```python
loss_original = CIFAR10_loss_fun(W) # the original loss

# lets see the effect of multiple step sizes
for step_size_log in [-10, -9, -8, -7, -6, -5,-4,-3,-2,-1]:
  step_size = 10 ** step_size_log
  # update in the negative direction of the gradient df since we wish our loss function to decrease, not increase.
  W_new = W - step_size * df # new position in the weight space
  loss_new = CIFAR10_loss_fun(W_new)
```

5. The variable `grad` holds the full gradient in the end.



##### 2. Analytic Gradient

to reduce the computational cost of `#1. Numerical Gradient`

* do not compute the gradient when it's not needed
* use analytic gradient notation and actually calculate gradients when needed
* **Debug**  using  numerical `gradient`

> unlike the numerical gradient it can be more error prone to implement, which is why in practice it is very common to
>
> 1. compute the analytic gradient
>
> 2. compare it to the numerical gradient
>    * check the correctness of your implementation.



<gradient>

1. count the number of classes that didn’t meet the desired margin
2. the data vector xi scaled by #1 number (counts)



* the row of W that corresponds to the `correct` class

![image-20210401195440383](/Users/JungHyunLah/Library/Application Support/typora-user-images/image-20210401195440383.png)

* other rows where `j≠yi`

![image-20210401195656748](/Users/JungHyunLah/Library/Application Support/typora-user-images/image-20210401195656748.png)



### Gradient Descent

the procedure of repeatedly evaluating the gradient and then performing a parameter update

```python
while True:
  weights_grad = evaluate_gradient(loss_fun, data, weights)
  weights += - step_size * weights_grad # perform parameter update
```



update weights to the opposite direction of the gradient descent to converge the model



#### minibatch

can't iterate through millions of train dataset over and over again to update the single W value for one time (high computing cost + time efficiency),

so to speed up the process of updating the W, use `mini-batch` as the estimate

* powers of 2 : usually among (32, 64, 128) 

> the examples in the training data are correlated
>
> 👉 the gradient from a mini-batch is a good approximation of the gradient of the full objective
>
>  👉 much faster convergence can be achieved in practice by evaluating the mini-batch gradients to perform more frequent parameter updates.

```python
while True:
  data_batch = sample_training_data(data, 256) # sample 256 examples
  weights_grad = evaluate_gradient(loss_fun, data_batch, weights)
  weights += - step_size * weights_grad # perform parameter update
```



#### Stochastic Gradient Descent (SGD)

> on-line gradient descent

the mini-batch contains only a single example

※ This is relatively less common to see because in practice, due to vectorized code optimizations, it can be computationally much more efficient to evaluate the gradient for 100 examples, than the gradient for one example 100 times



## Image Feature

1. pull out image features as groups,
2. concatenate them to make one image feature vector (feature representation vector),
3. feed it to the linear classifier (rather than feeding the raw pixels)



#### Color Histogram

divide the image using colours



#### Histogram of Oriented Gradients (HoG)

divide the pixel with dominant orientation



#### Bag of Words (Book of Visual Words : CodeBook)

1. get sample images, sample them into a bunch of tiny random crops,
2. cluster them using K means, etc to extract the main feature (e.g. color, orientation, etc.) of that crop (visual word)



### Here comes the ConvNet

* rather than just writing down the features

  = extracting features, concatenating them, and updating the linear classifier which is on the top and not the feature extractor itself,

* learn the features from the data

  = feed the raw data to the convolutional network, come up with the layers which are feature representations, train the weights using these to train the entire network