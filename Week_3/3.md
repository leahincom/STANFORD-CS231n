## Linear Classification

1. stretch out the image into column vector

2. make it into 10*1 column vector having scores to each class

=> larger score == closer to the answer

learning templates per class

how much that pixel influence that class

each row = a template for the class

dimension of the space = pixel intensity value



how to actually choose W and use the data

quantify how bad the W is => loss function



## Loss Function

how good or bad the W is

correct value (the least bad) => optimization procedure

choose W that minimizes the loss



### multi-class SVM Loss

sum over all the loss

s : predicted scores for the classes

s_yi = true category

s_j = false category + safety margin = 1

we want to have ... "much higher score for the true class"



hinge loss

![Screenshot 2021-03-26 at 19.54.02](/Users/JungHyunLah/git/STANFORD-CS231n/Week_3/Screenshot 2021-03-26 at 19.54.02.png)

- x axis = s_yi = true class
- y axis = loss

more true = less loss

at the end: loss = 0; already classified true value



**Debug** using  `C-1`



relative difference between scores

correct score is much greater than the incorrect scores

scaling up or down the entire W -> rescale all the scores correspondingly

*** course_note



quantify how bad are different mistakes (s_j)

weigh off different trade-offs of different types of mistakes

how much we care about different categories of errors



purpose : fitting to training data < performance on the test data

Occam's Razor : 'prefer the simple one'

=> regularization : simplify the W



* lambda

control the trade-off between data loss and optimization loss

=> important hyper-parameter to tune!



* hard constraint : constraint to contain high dimensional model

* soft constraint : if you want to contain high dimensional model, it has to overcome the penalty



### Softmax Classifier : Multinomial Logistic Regression

put much weight onto the scores



answer probability : want the probability of the true class to be 1

loss probability : want it to be closer to 0, flip the log = minus log

![image-20210326205520601](/Users/JungHyunLah/Library/Application Support/typora-user-images/image-20210326205520601.png)



in practice, it will never get the infinite probability cuz that of true class can't have the 0 probability



**Debug** using  `logC`



to measure the badness of the W, uses margin difference between correct and incorrect class



## Regularization

penalizing the complexity of the model rather than explicitely fit the training data



choose the type of regularization algo : problem-dependent



### L2 Regularization

penalizing the Euclidean norm of W (weight vector)

squared norm

how much the value of the x corresponds to the output class

measuring the model complexity by **how much the values are spreaded over all the entries is less complex**

opposite measuring criteria to L1 regularization

e.g. prefer [1,0,0,0]  > [0.25,0.25,0.25,0.25]



### L1 Regularization

encouraging sparsity

measuring the model complexity by **measuring the number of 0 entries(values) in the weight vector**

e.g. prefer  [0.25,0.25,0.25,0.25] > [1,0,0,0]





#### Elastic Net (L1 + L2)



#### Max Norm Regularization



#### Dropout

specific to deep learning





## Supervised Learning

![image-20210326210605027](/Users/JungHyunLah/Library/Application Support/typora-user-images/image-20210326210605027.png)

find the W that minimizes the loss



## Optimization



### Random Search

check all the possibilities



### Local Optimal = Slope

which way will be more downhill



**gradient** : same type of vector with x, partial derivatives

-> update the parameter values

1. positive : increase
2. negative : decrease

* do not compute it when it's not needed
* use analytic gradient : notation and actually calculate gradients when needed



**Debug** using  numerical `gradient`



## Gradient Descent

update weights to the opposite direction of the gradient descent to converge the model



* one of the most important hyperparamter

`Step Size` = Learning Rate

how far to update



can't iterate through millions of train dataset over and over again to update the W value for one time (high computing cost + time efficiency), so to speed up the process of updating the W :

### Stochastic Gradient Descent (SGD)

* minibatch (32, 64, 128) -> use as the estimate



## Image Feature

1. pull out image features as groups,

2. concatenate them to make one image feature vector (feature representation vector),
3. feed it to the linear classifier (rather than feeding the raw pixels)



### Color Histogram

divide the image using colours



### Histogram of Oriented Gradients (HoG)

divide the pixel with dominant orientation



### Bag of Words (Book of Visual Words : CodeBook)

1. get sample images, sample them into a bunch of tiny random crops,
2. cluster them using K means, etc to extract the main feature (e.g. color, orientation, etc.) of that crop (visual word)



**Here comes the ConvNet**

* rather than just writing down the features

  = extracting features, concatenating them, and updating the linear classifier which is on the top and not the feature extractor itself,

* learn the features from the data

  = feed the raw data to the convolutional network, come up with the layers which are feature representations, train the weights using these to train the entire network